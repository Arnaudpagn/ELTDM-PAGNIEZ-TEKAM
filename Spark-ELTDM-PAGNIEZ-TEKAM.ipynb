{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation du notebook\n",
    "\n",
    "Ce notebook présente le code produit par Arnaud PAGNIEZ et Yann TEKAM afin de reproduire un algorithme de calcul distribué appliqué à la technique d'Affinity Propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON    = jupyter-notebook\n",
      "PYSPARK_SUBMIT_ARGS      = \"--name\" \"PySparkShell\" \"pyspark-shell\" \n",
      "SPARK_CMD                = set PYSPARK_SUBMIT_ARGS=\"--name\" \"PySparkShell\" \"pyspark-shell\" && jupyter-notebook \n",
      "SPARK_ENV_LOADED         = 1\n",
      "SPARK_HOME               = C:\\Users\\<username>\\Spark\\bin\\..\n",
      "SPARK_JARS_DIR           = \"C:\\Users\\<username>\\Spark\\bin\\..\\jars\"\n",
      "SPARK_SCALA_VERSION      = 2.10\n",
      "_SPARK_CMD_USAGE         = Usage: bin\\pyspark.cmd [options]\n"
     ]
    }
   ],
   "source": [
    "### Test des variables d'environnements\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "for o, v in sorted(os.environ.items()):\n",
    "    if \"SPARK\" in o.upper():\n",
    "        print(\"{0:25}= {1}\".format(o, v.replace(os.environ[\"USERNAME\"], \"<username>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fichier.out.txt\\\\.part-00000.crc', 'file'),\n",
       " ('fichier.out.txt\\\\.part-00001.crc', 'file'),\n",
       " ('fichier.out.txt\\\\._SUCCESS.crc', 'file'),\n",
       " ('fichier.out.txt\\\\part-00000', 'file'),\n",
       " ('fichier.out.txt\\\\part-00001', 'file'),\n",
       " ('fichier.out.txt\\\\_SUCCESS', 'file'),\n",
       " ('fichier.out.txt', 'dir')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Petit test de fonctionnement de Spark\n",
    "\n",
    "from pyquickhelper.filehelper import remove_folder\n",
    "def clean(folder):\n",
    "    if os.path.exists(folder):\n",
    "        return remove_folder(folder)\n",
    "    else:\n",
    "        return []\n",
    "clean(\"fichier.out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000.crc',\n",
       " '.part-00001.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000',\n",
       " 'part-00001',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test de fonctionnement de Spark\n",
    "\n",
    "text_file = sc.textFile(\"Spark-ELTDM-PAGNIEZ-TEKAM.ipynb\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"fichier.out.txt\")\n",
    "os.listdir(\"fichier.out.txt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Définition de plusieurs fonctions utiles à la manipulation des RDD et à notre code en général\n",
    "\n",
    "def read_rdd(path, **options):\n",
    "    pat = os.path.join(path, \"part*\")\n",
    "    all_files = glob.glob(pat)\n",
    "    if len(all_files) == 0:\n",
    "        raise Exception(\"No file to read in '{0}'\".format(path))\n",
    "    merge = []\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            df = pandas.read_csv(f, header=None, **options)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Unable to read '{0}'\".format(f)) from e\n",
    "        merge.append(df)\n",
    "    if len(merge) == 0:\n",
    "        raise Exception(\"No file to read in '{0}'\".format(path))\n",
    "    concatenated_df = pandas.concat(merge, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "def extract_column(cols, row):\n",
    "    spl = row.split(\";\")\n",
    "    return [spl[i].strip() for i in cols]\n",
    "\n",
    "def filter_column(row):\n",
    "    spl = row.split(\";\")\n",
    "    return spl[-1].strip() != \"<=50K\"\n",
    "\n",
    "def filter_column_split(row):\n",
    "    return row[-1].strip() != \"<=50K\"\n",
    "\n",
    "def extract_age_rich(row):\n",
    "    spl = row.split(\";\")\n",
    "    target = spl[-1].strip()\n",
    "    age = float(spl[0])\n",
    "    return (age, target)\n",
    "\n",
    "def custom_agg(aggset):\n",
    "    temp = list([_[0] for _ in aggset])\n",
    "    return len(temp), sum(temp)\n",
    "\n",
    "def extract_column_and_multiply_row(n, row):\n",
    "    spl = row.split(\";\")\n",
    "    return [tuple(_.strip() for _ in spl)] * n\n",
    "\n",
    "def extract_value(x,i,j):\n",
    "    a = x.map(lambda row: extract_column([j],row))\n",
    "    return float(a.collect()[i][0])\n",
    "\n",
    "def convert_to_float_array(arr):\n",
    "\n",
    "    b = []\n",
    "\n",
    "    for e in arr:\n",
    "        if (isinstance(e,str)):\n",
    "            ind = e.index(',')\n",
    "            temp = float(e[:ind]) + 10**(-len(e) + ind + 1)*float(e[ind+1:])\n",
    "            b.append(temp)\n",
    "        else:\n",
    "            b.append(float(e))\n",
    "    \n",
    "    return np.array(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début de l'algorithme de Map/Reduce Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Importation de quelques packages (possible grâce au setup spark)\n",
    "import glob\n",
    "import pandas\n",
    "import os\n",
    "assert os.path.exists(\"AP_sample.csv\")\n",
    "import time\n",
    "\n",
    "#### Importation de toutes les données grâce à pandas\n",
    "\n",
    "df = pandas.read_csv(\"AP_sample.csv\", sep=\";\")\n",
    "df.set_index(df.columns[0],inplace = True)\n",
    "df.index.set_names('Index',inplace = True)\n",
    "\n",
    "### Création d'une table de test (petite)\n",
    "\n",
    "df_temp = df.iloc[:10,:]\n",
    "df_temp.to_csv('small_sample.csv', sep=';')\n",
    "\n",
    "### Création d'un rdd à partir de notre table de test \n",
    "\n",
    "rdd = sc.textFile(\"small_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Index,StringType,true),StructField(0,StringType,true),StructField(1,StringType,true),StructField(2,StringType,true),StructField(3,StringType,true),StructField(4,StringType,true),StructField(5,StringType,true),StructField(6,StringType,true),StructField(7,StringType,true),StructField(8,StringType,true),StructField(9,StringType,true),StructField(10,StringType,true),StructField(11,StringType,true),StructField(12,StringType,true),StructField(13,StringType,true),StructField(14,StringType,true),StructField(15,StringType,true),StructField(16,StringType,true),StructField(17,StringType,true),StructField(18,StringType,true),StructField(19,StringType,true),StructField(20,StringType,true),StructField(21,StringType,true),StructField(22,StringType,true),StructField(23,StringType,true),StructField(24,StringType,true),StructField(25,StringType,true),StructField(26,StringType,true),StructField(27,StringType,true),StructField(28,StringType,true),StructField(29,StringType,true),StructField(30,StringType,true),StructField(31,StringType,true),StructField(32,StringType,true),StructField(33,StringType,true),StructField(34,StringType,true),StructField(35,StringType,true),StructField(36,StringType,true),StructField(37,StringType,true),StructField(38,StringType,true),StructField(39,StringType,true),StructField(40,StringType,true),StructField(41,StringType,true),StructField(42,StringType,true),StructField(43,StringType,true),StructField(44,StringType,true),StructField(45,StringType,true),StructField(46,StringType,true),StructField(47,StringType,true),StructField(48,StringType,true),StructField(49,StringType,true),StructField(50,StringType,true),StructField(51,StringType,true),StructField(52,StringType,true),StructField(53,StringType,true),StructField(54,StringType,true),StructField(55,StringType,true),StructField(56,StringType,true),StructField(57,StringType,true),StructField(58,StringType,true),StructField(59,StringType,true),StructField(60,StringType,true),StructField(61,StringType,true),StructField(62,StringType,true),StructField(63,StringType,true),StructField(64,StringType,true),StructField(65,StringType,true),StructField(66,StringType,true),StructField(67,StringType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Premier test de création de Spark DataFrame\n",
    "\n",
    "lines = sc.textFile('small_sample.csv')\n",
    "rdd = lines.map(lambda x: x.split(';'))\n",
    "\n",
    "header = rdd.first()\n",
    "data = rdd.filter(lambda row : row != header).toDF(header)\n",
    "\n",
    "data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(0,LongType,true),StructField(1,LongType,true),StructField(2,LongType,true),StructField(3,LongType,true),StructField(4,LongType,true),StructField(5,LongType,true),StructField(6,LongType,true),StructField(7,LongType,true),StructField(8,LongType,true),StructField(9,LongType,true),StructField(10,LongType,true),StructField(11,LongType,true),StructField(12,LongType,true),StructField(13,LongType,true),StructField(14,LongType,true),StructField(15,LongType,true),StructField(16,LongType,true),StructField(17,LongType,true),StructField(18,LongType,true),StructField(19,LongType,true),StructField(20,LongType,true),StructField(21,LongType,true),StructField(22,LongType,true),StructField(23,LongType,true),StructField(24,LongType,true),StructField(25,LongType,true),StructField(26,LongType,true),StructField(27,LongType,true),StructField(28,LongType,true),StructField(29,LongType,true),StructField(30,LongType,true),StructField(31,LongType,true),StructField(32,LongType,true),StructField(33,LongType,true),StructField(34,LongType,true),StructField(35,LongType,true),StructField(36,StringType,true),StructField(37,LongType,true),StructField(38,LongType,true),StructField(39,LongType,true),StructField(40,LongType,true),StructField(41,LongType,true),StructField(42,LongType,true),StructField(43,LongType,true),StructField(44,LongType,true),StructField(45,LongType,true),StructField(46,LongType,true),StructField(47,LongType,true),StructField(48,LongType,true),StructField(49,LongType,true),StructField(50,LongType,true),StructField(51,LongType,true),StructField(52,LongType,true),StructField(53,LongType,true),StructField(54,LongType,true),StructField(55,LongType,true),StructField(56,LongType,true),StructField(57,LongType,true),StructField(58,LongType,true),StructField(59,LongType,true),StructField(60,LongType,true),StructField(61,LongType,true),StructField(62,LongType,true),StructField(63,LongType,true),StructField(64,LongType,true),StructField(65,LongType,true),StructField(66,LongType,true),StructField(67,LongType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Finalement solution plus courte et efficace pour créer une DataFrame Spark qui comporte le bon schéma de variablesS\n",
    "\n",
    "test = spark.createDataFrame(df_temp)\n",
    "test.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime = 1.5960910320281982 s\n",
      "243.0\n"
     ]
    }
   ],
   "source": [
    "### Mise à jour des dataframes pour gérer les éventuels str restants ###\n",
    "t0 = time.time()\n",
    "\n",
    "arr = df.values.copy()\n",
    "for i in range(len(df.index)):\n",
    "    arr[i,:] = convert_to_float_array(arr[i,:])\n",
    "    \n",
    "df = pandas.DataFrame(arr)\n",
    "   \n",
    "print(\"runtime = {} s\".format(time.time() - t0))\n",
    "print(df.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Création du dictionnaire des vecteurs X de notre dataset grâce à Spark\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "len_samples = df_temp.shape[0]\n",
    "nb_col = df_temp.shape[1]\n",
    "\n",
    "dict_vectors = {}\n",
    "\n",
    "for i in range(len_samples):\n",
    "    arr = convert_to_float_array(df_temp.iloc[i].values)\n",
    "    vector = Vectors.dense(arr)\n",
    "    \n",
    "    dict_vectors[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime = 0.005000114440917969 s\n"
     ]
    }
   ],
   "source": [
    "## Initialisation de la matrice d'affinités \n",
    "### Mise en place de l'algorithme AP dans une fonction dédiée\n",
    "\n",
    "t0 =time.time()\n",
    "\n",
    "def affinity_propagation(df,n_iter=100,lamb=0.5,n_convergence_iter=5,pref = \"min\"):\n",
    "    \n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "    len_samples = df.shape[0]\n",
    "    nb_col = df.shape[1]\n",
    "\n",
    "    dict_vectors = {}\n",
    "\n",
    "    for i in range(len_samples):\n",
    "        arr = convert_to_float_array(df.iloc[i].values)\n",
    "        vector = Vectors.dense(arr)\n",
    "\n",
    "        dict_vectors[i] = vector\n",
    "    \n",
    "    ### Initialisation des matrices de l'algorithme\n",
    "    \n",
    "    distance_matrix = np.zeros((len_samples,len_samples))\n",
    "    availability_matrix = np.zeros((len_samples,len_samples))\n",
    "    responsability_matrix = np.zeros((len_samples,len_samples))\n",
    "    \n",
    "    ### Calcul de matrice des distances grâce à Spark\n",
    "    \n",
    "    for i in range(len_samples):\n",
    "        for j in range(len_samples):\n",
    "            distance_matrix[i,j] = -np.sqrt(dict_vectors[i].squared_distance(dict_vectors[j]))\n",
    "\n",
    "    ## Création des préférences\n",
    "    \n",
    "    if( pref == \"min\"):\n",
    "        preferences = np.min(distance_matrix.flatten())\n",
    "    else:\n",
    "        preferences = np.median(distance_matrix.flatten())\n",
    "\n",
    "    for i in range(len_samples):\n",
    "        distance_matrix[i,i] =  preferences \n",
    "\n",
    "    nb_clusters = []\n",
    "    \n",
    "    #### Itérations des mises à jour jusqu'a convergence\n",
    "    \n",
    "    for u in range(n_iter):\n",
    "\n",
    "        for i in range(len_samples):\n",
    "            \n",
    "            arr_temp = [availability_matrix[i,k] + distance_matrix[i,k] for k in range(len_samples)]\n",
    "            value_temp = distance_matrix[i,i] - np.max([distance_matrix[i,k] for k in range(len_samples) if k!=i])\n",
    "            \n",
    "            for j in range(len_samples):\n",
    "                \n",
    "                if (i != j):\n",
    "                    temp = distance_matrix[i,j] - np.max(np.delete(arr_temp,j))\n",
    "                    responsability_matrix[i,j] = lamb * responsability_matrix[i,j] + (1-lamb) * temp\n",
    "                else:\n",
    "                    responsability_matrix[i,j] = lamb * responsability_matrix[i,j] + (1-lamb) * value_temp\n",
    "                                                \n",
    "        for i in range(len_samples):\n",
    "                                                         \n",
    "            value_temp = np.sum([np.max([0,responsability_matrix[k,i]]) for k in range(len_samples) if (k!=i)])\n",
    "            \n",
    "            for j in range(len_samples):\n",
    "                                                         \n",
    "                if (i != j):\n",
    "                    temp = min(0,responsability_matrix[j,j] + \\\n",
    "                            np.sum([np.max([0,responsability_matrix[k,j]]) for k in range(len_samples) if (k!=j and k!=i)]))\n",
    "                    availability_matrix[i,j] = lamb * availability_matrix[i,j] + (1-lamb)*temp\n",
    "                else:\n",
    "                    \n",
    "                    availability_matrix[i,j] = lamb * availability_matrix[i,j] + (1-lamb)*value_temp\n",
    "\n",
    "\n",
    "        total = availability_matrix + responsability_matrix\n",
    "        K = np.sum(np.diag(total) > 0 )\n",
    "\n",
    "        nb_clusters.append(K)\n",
    "\n",
    "        if(len(nb_clusters)>n_convergence_iter):\n",
    "            test = np.sum(nb_clusters[u - n_convergence_iter:] == K)\n",
    "            if (test == n_convergence_iter):\n",
    "                print('Algo Converged after {} iterations'.format(u))\n",
    "                break\n",
    "    \n",
    "    #### Identification des clusters et mise en forme du résultat final\n",
    "    \n",
    "    I = np.where(np.diag(availability_matrix + responsability_matrix) > 0)[0]\n",
    "    K = I.size \n",
    "\n",
    "    if K > 0:\n",
    "        c = np.argmax(distance_matrix[:, I], axis=1)\n",
    "        c[I] = np.arange(K)  # Identification des clusters\n",
    "        \n",
    "        for k in range(K):\n",
    "            ii = np.where(c == k)[0]\n",
    "            j = np.argmax(np.sum(distance_matrix[ii[:, np.newaxis], ii], axis=0))\n",
    "            I[k] = ii[j]\n",
    "\n",
    "        c = np.argmax(distance_matrix[:, I], axis=1)\n",
    "        c[I] = np.arange(K)\n",
    "        labels = I[c]\n",
    "        # Création des labels par points\n",
    "        cluster_centers_indices = np.unique(labels)\n",
    "        labels = np.searchsorted(cluster_centers_indices, labels)\n",
    "    else:\n",
    "        labels = np.empty((len_samples, 1))\n",
    "        cluster_centers_indices = None\n",
    "        labels.fill(np.nan)\n",
    "    \n",
    "    return cluster_centers_indices, labels, u, preferences\n",
    "\n",
    "print(\"runtime = {} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo Converged after 46 iterations\n",
      "runtime = 1.9741120338439941 s\n"
     ]
    }
   ],
   "source": [
    "### Test de la fonction\n",
    "\n",
    "t0 = time.time()\n",
    "a,b,c,d = affinity_propagation(df_temp,100,0.9,5)\n",
    "print('runtime = {} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction permettant de créer la hash table <cluster center, cluster members>\n",
    "\n",
    "def local_computation (df,n_iter=100,n_convergence_iter=5,damp=0.5,pref = \"min\"):\n",
    "    \n",
    "    cluster_centers_indices, labels, n_iter, pref = affinity_propagation(df,n_iter,damp,n_convergence_iter,pref)\n",
    "    \n",
    "    points_centers = {}\n",
    "    \n",
    "    if(cluster_centers_indices != None):\n",
    "        if(len(cluster_centers_indices)>0):\n",
    "\n",
    "            for e in cluster_centers_indices:\n",
    "\n",
    "                lab = np.searchsorted(cluster_centers_indices,e)\n",
    "\n",
    "                points_centers[e] = np.where(labels == lab)[0]\n",
    "    else: \n",
    "        return(\"No cluster centers\")\n",
    "    \n",
    "    return points_centers, pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo Converged after 21 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xelle\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo Converged after 24 iterations\n",
      "runtime = 130.87148594856262 s\n"
     ]
    }
   ],
   "source": [
    "#### Test de la fonction de hachage ###\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df1 = df.iloc[:50,:]\n",
    "df2 = df.iloc[50:100,:]\n",
    "\n",
    "pts_centers1, pref = local_computation(df1,100,5,0.9,\"min\")\n",
    "pts_centers2, pref = local_computation(df2,100,5,0.9,\"min\")\n",
    "\n",
    "print(\"runtime = {} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], dtype=int32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Table de hachage 1 ###\n",
    "\n",
    "pts_centers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: array([ 0,  1,  7, 10, 11, 12, 14, 15, 16, 17, 20, 21, 22, 23, 24, 30, 31,\n",
       "        34, 40, 41, 43, 44, 46, 47, 48], dtype=int32),\n",
       " 9: array([ 2,  3,  4,  5,  6,  8,  9, 13, 18, 19, 25, 26, 27, 28, 29, 32, 33,\n",
       "        35, 36, 37, 38, 39, 42, 45, 49], dtype=int32)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Table de hachage 2 ####\n",
    "\n",
    "pts_centers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Fonction permettant de rassembler les résultats des différents mappers ####\n",
    "\n",
    "def merge (df1,df2, points_centers1, points_centers2, pref):\n",
    "    \n",
    "    if(isinstance(points_centers1,str) or isinstance(points_centers2,str)):\n",
    "        print(\"No clusters to merge\")\n",
    "        return 0.0\n",
    "    \n",
    "    keys1 = [e for e in points_centers1.keys()]\n",
    "    keys2 = [e for e in points_centers2.keys()]\n",
    "    \n",
    "    centers_1 = df1.iloc[keys1, :].values\n",
    "    centers_2 = df2.iloc[keys2, :].values\n",
    "    \n",
    "    n1 = df1.shape[0]\n",
    "    n2 = df2.shape[0]\n",
    "    \n",
    "    df_full = pandas.concat([df1,df2])\n",
    "    \n",
    "    dict_vectors_1 = {}\n",
    "    dict_vectors_2 = {}\n",
    "    \n",
    "    for i in keys1:\n",
    "        arr = convert_to_float_array(df1.iloc[i].values)\n",
    "        vector = Vectors.dense(arr)\n",
    "\n",
    "        dict_vectors_1[i] = vector\n",
    "    \n",
    "    for i in keys2:\n",
    "        arr = convert_to_float_array(df2.iloc[i].values)\n",
    "        vector = Vectors.dense(arr)\n",
    "\n",
    "        dict_vectors_2[i] = vector\n",
    "    \n",
    "    distance_centers = np.zeros([len(keys1),len(keys2)])\n",
    "    \n",
    "    threshold = 0.5*np.abs(pref)\n",
    "    \n",
    "    to_merge = []\n",
    "    \n",
    "    for i in range(len(keys1)):\n",
    "        for j in range(len(keys2)):\n",
    "            \n",
    "            a = np.sqrt(dict_vectors_1[keys1[i]].squared_distance(dict_vectors_2[keys2[j]]))\n",
    "            distance_centers[i,j] = a\n",
    "            \n",
    "            if(a < threshold):\n",
    "                if(len(to_merge)>0.0):\n",
    "                    temp1 = [e[0] for e in to_merge]\n",
    "                    temp2 = [e[1] for e in to_merge]\n",
    "                    \n",
    "                    if not((i in temp1) or (j in temp2)):\n",
    "                        to_merge.append([i,j])\n",
    "                else:\n",
    "                    to_merge.append([i,j])\n",
    "                \n",
    "    new_centers = {}\n",
    "    \n",
    "    for i in range(len(keys1)):\n",
    "        key = keys1[i]\n",
    "        if not(i in [e[0] for e in to_merge]):\n",
    "            new_centers[key] = points_centers1[key]\n",
    "    for i in range(len(keys2)):\n",
    "        key = keys2[i]\n",
    "        if not(i in [e[1] for e in to_merge]):\n",
    "            new_centers[key + n1] = np.array([ e + n1 for e in points_centers2[key]])\n",
    "                \n",
    "    if (len(to_merge) > 0):\n",
    "        \n",
    "        \n",
    "        for i,j in to_merge:\n",
    "            \n",
    "            all_points = [e for e in points_centers1[keys1[i]]] + [e + n1 for e in points_centers2[keys2[j]]]\n",
    "            \n",
    "            print(keys1[i],keys2[j])\n",
    "            new_center = np.mean(np.array([convert_to_float_array(df1.iloc[keys1[i]].values),convert_to_float_array(df2.iloc[keys2[j]].values)]),axis = 0)\n",
    "            \n",
    "            new_centers[len(df_full)] = np.array(all_points)\n",
    "            \n",
    "            df_full = df_full.append(pandas.Series(new_center), ignore_index = True)\n",
    "            df_full = df_full.drop(df_full.index[[i,n1 + j]])\n",
    "        \n",
    "    \n",
    "    return distance_centers, df_full, new_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 9\n",
      "runtime = 0.010000944137573242 s\n",
      "[[ 60.61725497  62.98258807]]\n"
     ]
    }
   ],
   "source": [
    "## Test de la fonction ####\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "a, b, final_hash = merge(df1,df2,pts_centers1,pts_centers2,pref)\n",
    "\n",
    "print(\"runtime = {} s\".format(time.time() - t0))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{57: array([50, 51, 57, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 73, 74, 80, 81,\n",
       "        84, 90, 91, 93, 94, 96, 97, 98], dtype=int32),\n",
       " 100: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52,\n",
       "        53, 54, 55, 56, 58, 59, 63, 68, 69, 75, 76, 77, 78, 79, 82, 83, 85,\n",
       "        86, 87, 88, 89, 92, 95, 99], dtype=int32)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], dtype=int32)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Table de hachage 1 ###\n",
    "pts_centers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: array([ 0,  1,  7, 10, 11, 12, 14, 15, 16, 17, 20, 21, 22, 23, 24, 30, 31,\n",
       "        34, 40, 41, 43, 44, 46, 47, 48], dtype=int32),\n",
       " 9: array([ 2,  3,  4,  5,  6,  8,  9, 13, 18, 19, 25, 26, 27, 28, 29, 32, 33,\n",
       "        35, 36, 37, 38, 39, 42, 45, 49], dtype=int32)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Table de hachage 2 ###\n",
    "pts_centers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{57: array([50, 51, 57, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 73, 74, 80, 81,\n",
       "        84, 90, 91, 93, 94, 96, 97, 98], dtype=int32),\n",
       " 100: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52,\n",
       "        53, 54, 55, 56, 58, 59, 63, 68, 69, 75, 76, 77, 78, 79, 82, 83, 85,\n",
       "        86, 87, 88, 89, 92, 95, 99], dtype=int32)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Table de hachage résultante ###\n",
    "\n",
    "final_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Ce notebook présente donc les fonctions nécessaires au calcul distribué appliqué à l'Affinity Propagation et implémente un exemple sur lequel on voit bien que la table de hachage finale correspond au résultat attendu.\n",
    "\n",
    "Il resterait maintenant à implémenter les fonctions présentées ci-dessus sur un cluster réel afin de paralléliser de manière effective la technique d'Affinity Propagation sur la base de données considérée."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
